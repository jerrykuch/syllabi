# Catalog Description

## BUILDING THE DATA PIPELINE

This course focuses on the process used to acquire, store and process data for downstream analysis. You'll analyze and compare available technologies in order to make informed decisions as data engineers. You'll also learn how to build data processing workflows using several data stack platforms and design and run data pipelines for real-world business use cases.

## WHAT YOU'LL LEARN

- How a data lake can enhance the usability of your organization’s data
- Batch and streaming processing using Spark, Flink and other processing tools
- How to use Kafka to enable low-latency and real-time processing
- Data acquisition and modeling techniques • Pipeline design and integration

## GET HANDS-ON EXPERIENCE

- Organize and store data in a data lake and handle updates and changes to your data
- Use Spark to connect to different data sources and process batch and streaming data
- Design and build a complete end-to-end data pipeline to support a realistic business case


# Course Outline
- Welcome_Intro
- NoSQL Part 1 (38 slides) (*TODO* Add a written/conceptual assignment)
- HBase (17 slides)  (w/ written/conceptual + exercises in Docker assignment)
- NoSQL_What_We_Lose (2 slides)
- Cassandra
- Data Pipelines and Platforms (was old week 2)
- Modernizing the DW (was old week 3)
- Beyond the DW--The Lakehouse (was Chal. w/ Mod. DW in old week 4)
- Delta Lake
- Streaming Analytics, Unified Log Model, and Kafka (was old week 6)
- Processing Streaming Data (was old week 7)
- The Data Platform Serving Layer (was old week 9)
- Production Considerations for Data Platforms (was old week 10)
- Processing Streaming Data 2, Alternatives (was old week 8)
